{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-1k3IxSTAO2",
        "outputId": "cefcf0c1-e95b-42c3-c401-5a24bfb0b845"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import traceback\n",
        "from typing import List, Dict, Tuple, Optional, Any, Union\n",
        "from dataclasses import dataclass, field\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import multiprocessing\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"predictive_policing\")\n",
        "\n",
        "# Configuration class using dataclass for type hints and defaults\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Paths\n",
        "    base_dir: str = \"/content/drive/MyDrive/predictive_policing\"\n",
        "    data_dir: str = field(default=\"data\")\n",
        "    raw_dir: str = field(default=\"raw\")\n",
        "    processed_dir: str = field(default=\"processed\")\n",
        "    models_dir: str = field(default=\"models\")\n",
        "    results_dir: str = field(default=\"results\")\n",
        "\n",
        "    # Data files\n",
        "    chicago_crime_file: str = \"chicago_crime.csv\"\n",
        "    uci_communities_file: str = \"uci_communities.csv\"\n",
        "\n",
        "    # Model parameters\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = 42\n",
        "    target_column: str = \"crime_rate\"\n",
        "\n",
        "    # Fairness parameters\n",
        "    # These are potential protected attributes - will be validated against available columns\n",
        "    protected_attributes: List[str] = field(default_factory=lambda: [\n",
        "        \"racepctblack\", \"racePctWhite\", \"racePctAsian\",\n",
        "        \"racePctHisp\", \"pctWPubAsst\", \"medIncome\"\n",
        "    ])\n",
        "    # Alternative demographic proxies if primary attributes aren't available\n",
        "    demographic_proxies: List[str] = field(default_factory=lambda: [\n",
        "        \"community_area\", \"community_name\", \"area_code\", \"zipcode\"\n",
        "    ])\n",
        "\n",
        "    # Processing\n",
        "    n_cores: int = field(default_factory=lambda: max(1, multiprocessing.cpu_count() - 1))\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Initialize derived paths after construction\"\"\"\n",
        "        # Create full paths by joining with base_dir\n",
        "        self.data_dir = os.path.join(self.base_dir, self.data_dir)\n",
        "        self.raw_dir = os.path.join(self.data_dir, self.raw_dir)\n",
        "        self.processed_dir = os.path.join(self.data_dir, self.processed_dir)\n",
        "        self.models_dir = os.path.join(self.data_dir, self.models_dir)\n",
        "        self.results_dir = os.path.join(self.data_dir, self.results_dir)\n",
        "\n",
        "        # Make sure directories exist\n",
        "        for directory in [self.data_dir, self.raw_dir, self.processed_dir,\n",
        "                          self.models_dir, self.results_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data loading, cleaning, and preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(f\"{__name__}.DataProcessor\")\n",
        "\n",
        "    def load_csv(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"Load a CSV file with error handling.\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(filepath)\n",
        "            logger.info(f\"Loaded {filepath} with shape {df.shape}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading {filepath}: {e}\")\n",
        "            # Return an empty DataFrame with a default structure if file not found\n",
        "            if isinstance(e, FileNotFoundError):\n",
        "                logger.warning(f\"Creating empty DataFrame as {filepath} was not found\")\n",
        "                return pd.DataFrame()\n",
        "            raise\n",
        "\n",
        "    def clean_chicago_crime(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Clean and preprocess Chicago crime data.\"\"\"\n",
        "        logger.info(f\"Cleaning Chicago crime data from {os.path.join(self.config.raw_dir, self.config.chicago_crime_file)}\")\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            logger.warning(\"Empty crime data, creating minimal synthetic data for testing\")\n",
        "            # Create minimal synthetic data\n",
        "            synthetic_df = pd.DataFrame({\n",
        "                'id': range(1000),\n",
        "                'community_area': np.random.randint(1, 78, 1000),\n",
        "                'primary_type': np.random.choice(['THEFT', 'BATTERY', 'ASSAULT'], 1000),\n",
        "                'date': pd.date_range(start='2020-01-01', periods=1000)\n",
        "            })\n",
        "            return synthetic_df\n",
        "\n",
        "        # Make a copy to avoid modifying the original\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic cleaning steps\n",
        "        # 1. Drop duplicates\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # 2. Convert date columns if present\n",
        "        date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
        "        for col in date_columns:\n",
        "            try:\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # 3. Handle missing values based on column type\n",
        "        for col in df.columns:\n",
        "            # Skip date columns already handled\n",
        "            if col in date_columns:\n",
        "                continue\n",
        "\n",
        "            if df[col].dtype == 'object':\n",
        "                # For categorical columns, fill with 'Unknown'\n",
        "                df[col] = df[col].fillna('Unknown')\n",
        "            else:\n",
        "                # For numeric columns, fill with median\n",
        "                df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)\n",
        "\n",
        "        logger.info(f\"Cleaned Chicago data shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def clean_uci_communities(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Clean and preprocess UCI Communities data.\"\"\"\n",
        "        logger.info(f\"Cleaning UCI Communities data from {os.path.join(self.config.raw_dir, self.config.uci_communities_file)}\")\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            logger.warning(\"Empty communities data, creating minimal synthetic data for testing\")\n",
        "            # Create minimal synthetic data\n",
        "            synthetic_df = pd.DataFrame({\n",
        "                'community_area': range(1, 101),\n",
        "                'population': np.random.randint(5000, 100000, 100),\n",
        "                'median_income': np.random.randint(30000, 100000, 100),\n",
        "                'community_name': [f\"Area_{i}\" for i in range(1, 101)]\n",
        "            })\n",
        "            return synthetic_df\n",
        "\n",
        "        # Make a copy to avoid modifying the original\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic cleaning steps\n",
        "        # 1. Drop duplicates\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # 2. Handle missing values\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # For categorical columns, fill with 'Unknown'\n",
        "                df[col] = df[col].fillna('Unknown')\n",
        "            else:\n",
        "                # For numeric columns, fill with median\n",
        "                df[col] = df[col].fillna(df[col].median() if not pd.isna(df[col].median()) else 0)\n",
        "\n",
        "        logger.info(f\"Cleaned UCI data shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create features from crime data for modeling.\"\"\"\n",
        "        logger.info(\"Creating features from crime data\")\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            logger.warning(\"Empty DataFrame, cannot create features\")\n",
        "            return df\n",
        "\n",
        "        # Make a copy to avoid modifying the original\n",
        "        df = df.copy()\n",
        "\n",
        "        # Example feature engineering - adjust based on available columns\n",
        "        # If date column exists, extract time-based features\n",
        "        date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
        "        if date_columns:\n",
        "            date_col = date_columns[0]\n",
        "            if pd.api.types.is_datetime64_dtype(df[date_col]):\n",
        "                df['month'] = df[date_col].dt.month\n",
        "                df['day_of_week'] = df[date_col].dt.dayofweek\n",
        "                if hasattr(df[date_col].dt, 'hour'):  # Check if time component exists\n",
        "                    df['hour'] = df[date_col].dt.hour\n",
        "                df['is_weekend'] = df[date_col].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "\n",
        "        # If crime type exists, create dummy variables\n",
        "        if 'primary_type' in df.columns:\n",
        "            # Get top 5 crime types for dummy variables\n",
        "            top_crimes = df['primary_type'].value_counts().nlargest(5).index\n",
        "            for crime in top_crimes:\n",
        "                crime_col_name = f'is_{crime.lower().replace(\" \", \"_\")}'\n",
        "                # Ensure no name conflicts\n",
        "                if crime_col_name not in df.columns:\n",
        "                    df[crime_col_name] = (df['primary_type'] == crime).astype(int)\n",
        "\n",
        "        logger.info(f\"Created features, new shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def merge_datasets(self, crime_df: pd.DataFrame, communities_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Merge crime and communities datasets.\"\"\"\n",
        "        logger.info(\"Merging datasets\")\n",
        "\n",
        "        # Check if either DataFrame is empty\n",
        "        if crime_df.empty:\n",
        "            logger.warning(\"Crime DataFrame is empty, using communities data only\")\n",
        "            # Add a synthetic crime count\n",
        "            if not communities_df.empty:\n",
        "                communities_df['crime_count'] = np.random.randint(10, 1000, size=len(communities_df))\n",
        "                communities_df['crime_rate'] = communities_df['crime_count'] / communities_df['crime_count'].max()\n",
        "            return communities_df\n",
        "\n",
        "        if communities_df.empty:\n",
        "            logger.warning(\"Communities DataFrame is empty, using crime data only\")\n",
        "            # Aggregate crime data to provide a minimal merged dataset\n",
        "            if 'community_area' in crime_df.columns:\n",
        "                # Group by community area and count crimes\n",
        "                crime_counts = crime_df.groupby('community_area').size().reset_index(name='crime_count')\n",
        "                crime_counts['crime_rate'] = crime_counts['crime_count'] / crime_counts['crime_count'].max()\n",
        "                return crime_counts\n",
        "            return crime_df\n",
        "\n",
        "        # Identify potential join columns\n",
        "        possible_join_columns = [\n",
        "            'community_area', 'community_id', 'area_code',\n",
        "            'neighborhood', 'district', 'zip_code', 'community'\n",
        "        ]\n",
        "\n",
        "        # Find columns present in both datasets\n",
        "        join_columns = [col for col in possible_join_columns\n",
        "                        if col in crime_df.columns and col in communities_df.columns]\n",
        "\n",
        "        # If no common columns found, try to create a join\n",
        "        if not join_columns:\n",
        "            logger.warning(\"No common join columns found. Creating synthetic join.\")\n",
        "\n",
        "            # Create a new unique ID for both datasets\n",
        "            crime_df = crime_df.copy()\n",
        "            communities_df = communities_df.copy()\n",
        "\n",
        "            # If community_area exists in crime data but not communities data,\n",
        "            # create it in communities data\n",
        "            if 'community_area' in crime_df.columns and 'community_area' not in communities_df.columns:\n",
        "                # Get unique community areas from crime data\n",
        "                unique_areas = crime_df['community_area'].unique()\n",
        "\n",
        "                # If communities DataFrame has fewer rows than unique areas, expand it\n",
        "                if len(communities_df) < len(unique_areas):\n",
        "                    # Duplicate rows to match number of unique areas\n",
        "                    communities_df = pd.concat([communities_df] * (len(unique_areas) // len(communities_df) + 1))\n",
        "                    communities_df = communities_df.iloc[:len(unique_areas)].reset_index(drop=True)\n",
        "\n",
        "                # Assign community areas to communities DataFrame\n",
        "                communities_df['community_area'] = unique_areas[:len(communities_df)]\n",
        "\n",
        "                # Now merge on community_area\n",
        "                merged_df = communities_df.merge(\n",
        "                    crime_df.groupby('community_area').size().reset_index(name='crime_count'),\n",
        "                    on='community_area',\n",
        "                    how='left'\n",
        "                )\n",
        "                merged_df['crime_count'] = merged_df['crime_count'].fillna(0)\n",
        "\n",
        "                # Calculate crime rate\n",
        "                merged_df['crime_rate'] = merged_df['crime_count'] / merged_df['crime_count'].max()\n",
        "\n",
        "                logger.info(f\"Created synthetic join on community_area, merged shape: {merged_df.shape}\")\n",
        "                return merged_df\n",
        "\n",
        "            # Otherwise, create a completely synthetic join\n",
        "            logger.warning(\"Creating fully synthetic join - for development only\")\n",
        "\n",
        "            # Create a join key in both DataFrames\n",
        "            n_communities = min(len(communities_df), 100)  # Limit to at most 100 communities\n",
        "            crime_df['_join_key'] = np.random.randint(0, n_communities, size=len(crime_df))\n",
        "            communities_df['_join_key'] = range(n_communities)\n",
        "            communities_df = communities_df.iloc[:n_communities]\n",
        "\n",
        "            # Group crime data by join key to get crime counts\n",
        "            crime_counts = crime_df.groupby('_join_key').size().reset_index(name='crime_count')\n",
        "\n",
        "            # Merge\n",
        "            merged_df = communities_df.merge(crime_counts, on='_join_key', how='left')\n",
        "\n",
        "            # Fill missing crime counts with 0\n",
        "            merged_df['crime_count'] = merged_df['crime_count'].fillna(0)\n",
        "\n",
        "            # Calculate crime rate\n",
        "            merged_df['crime_rate'] = merged_df['crime_count'] / merged_df['crime_count'].max()\n",
        "\n",
        "            # Remove join key\n",
        "            merged_df = merged_df.drop('_join_key', axis=1)\n",
        "\n",
        "            logger.info(f\"Created fully synthetic join, merged shape: {merged_df.shape}\")\n",
        "            return merged_df\n",
        "\n",
        "        # Use the first available join column\n",
        "        join_col = join_columns[0]\n",
        "        logger.info(f\"Joining datasets on '{join_col}'\")\n",
        "\n",
        "        # Check if the join column has unique values in the communities DataFrame\n",
        "        if not communities_df[join_col].is_unique:\n",
        "            logger.warning(f\"Join column '{join_col}' is not unique in communities data - this may cause data duplication\")\n",
        "\n",
        "        # Aggregate crime data by join column\n",
        "        crime_grouped = crime_df.groupby(join_col).size().reset_index(name='crime_count')\n",
        "\n",
        "        # Merge with communities data\n",
        "        merged_df = communities_df.merge(crime_grouped, on=join_col, how='left')\n",
        "\n",
        "        # Fill missing crime counts with 0\n",
        "        merged_df['crime_count'] = merged_df['crime_count'].fillna(0)\n",
        "\n",
        "        # Calculate crime rate\n",
        "        if 'population' in merged_df.columns:\n",
        "            # Ensure population is never 0 to avoid division by zero\n",
        "            merged_df['population'] = merged_df['population'].replace(0, 1)\n",
        "            merged_df['crime_rate'] = merged_df['crime_count'] / merged_df['population']\n",
        "        else:\n",
        "            # No population data, normalize by maximum crime count\n",
        "            merged_df['crime_rate'] = merged_df['crime_count'] / merged_df['crime_count'].max()\n",
        "\n",
        "        logger.info(f\"Merged data shape: {merged_df.shape}\")\n",
        "        return merged_df\n",
        "\n",
        "    def validate_protected_attributes(self, df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"Validate and find protected attributes that exist in the data.\"\"\"\n",
        "        # Check which configured protected attributes are in the dataframe\n",
        "        available_protected = [attr for attr in self.config.protected_attributes\n",
        "                              if attr in df.columns]\n",
        "\n",
        "        # If none found, check for demographic proxies\n",
        "        if not available_protected:\n",
        "            available_protected = [attr for attr in self.config.demographic_proxies\n",
        "                                  if attr in df.columns]\n",
        "\n",
        "            if available_protected:\n",
        "                logger.info(f\"Using proxy demographic columns: {available_protected}\")\n",
        "            else:\n",
        "                logger.warning(\"No protected attributes or proxies found in the data\")\n",
        "                # If no attributes found, try to find any numeric column that could serve as proxy\n",
        "                numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "                if numeric_cols:\n",
        "                    proxy_col = numeric_cols[0]\n",
        "                    logger.warning(f\"Using '{proxy_col}' as fallback demographic proxy\")\n",
        "                    available_protected = [proxy_col]\n",
        "        else:\n",
        "            logger.info(f\"Using protected attributes: {available_protected}\")\n",
        "\n",
        "        return available_protected\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Handles model training, evaluation, and fairness analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(f\"{__name__}.ModelTrainer\")\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"Prepare features and target for modeling.\"\"\"\n",
        "        if df.empty:\n",
        "            logger.error(\"Cannot prepare data - DataFrame is empty\")\n",
        "            raise ValueError(\"Empty DataFrame provided to prepare_data\")\n",
        "\n",
        "        # Create target if it doesn't exist\n",
        "        if target_col not in df.columns:\n",
        "            logger.warning(f\"Target column '{target_col}' not found in data - creating synthetic target\")\n",
        "            df[target_col] = np.random.random(len(df))\n",
        "\n",
        "        # Select features - exclude obvious non-predictive columns\n",
        "        exclude_cols = ['id', 'case_number', 'date', 'block', 'iucr', 'community_name',\n",
        "                       'latitude', 'longitude', 'beat', 'district', 'ward',\n",
        "                       target_col]\n",
        "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Handle categorical columns if any\n",
        "        categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        if categorical_cols:\n",
        "            logger.info(f\"Dropping {len(categorical_cols)} categorical columns (would need one-hot encoding)\")\n",
        "            feature_cols = [col for col in feature_cols if col not in categorical_cols]\n",
        "            if not feature_cols:\n",
        "                # If all features were categorical, create simple numeric features\n",
        "                logger.warning(\"No numeric features available - creating simple index feature\")\n",
        "                df['index_feature'] = range(len(df))\n",
        "                feature_cols = ['index_feature']\n",
        "\n",
        "        # Make sure we have some features\n",
        "        if not feature_cols:\n",
        "            logger.warning(\"No usable features found - creating simple index feature\")\n",
        "            df['index_feature'] = range(len(df))\n",
        "            feature_cols = ['index_feature']\n",
        "\n",
        "        # Extract features and target\n",
        "        X = df[feature_cols]\n",
        "        y = df[target_col]\n",
        "\n",
        "        logger.info(f\"Prepared data with {X.shape[1]} features and {y.shape[0]} samples\")\n",
        "        return X, y\n",
        "\n",
        "    def train_model(self, X: pd.DataFrame, y: pd.Series, model_type: str = 'xgboost') -> Dict:\n",
        "        \"\"\"Train a predictive model.\"\"\"\n",
        "        if X.empty or len(y) == 0:\n",
        "            logger.error(\"Cannot train model - empty feature matrix or target vector\")\n",
        "            raise ValueError(\"Empty data provided to train_model\")\n",
        "\n",
        "        logger.info(f\"Training {model_type} model\")\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=self.config.test_size, random_state=self.config.random_state\n",
        "        )\n",
        "\n",
        "        # Normalize features - handle edge cases and errors\n",
        "        try:\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error scaling features: {e} - using unscaled features\")\n",
        "            X_train_scaled = X_train.to_numpy()\n",
        "            X_test_scaled = X_test.to_numpy()\n",
        "\n",
        "        # Train model with error handling\n",
        "        try:\n",
        "            if model_type.lower() == 'xgboost':\n",
        "                model = xgb.XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=5,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.config.random_state\n",
        "                )\n",
        "            else:\n",
        "                # Default to Random Forest\n",
        "                model = RandomForestRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=10,\n",
        "                    random_state=self.config.random_state\n",
        "                )\n",
        "\n",
        "            # Fit model\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Evaluate\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            logger.info(f\"Model performance - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training model: {e}\")\n",
        "            logger.info(\"Switching to backup simple model\")\n",
        "\n",
        "            # Create a simple fallback model (mean predictor)\n",
        "            class SimpleMeanPredictor:\n",
        "                def __init__(self, mean_value):\n",
        "                    self.mean_value = mean_value\n",
        "\n",
        "                def predict(self, X):\n",
        "                    return np.full(X.shape[0], self.mean_value)\n",
        "\n",
        "                def get_feature_importances(self):\n",
        "                    return np.ones(X.shape[1]) / X.shape[1]\n",
        "\n",
        "            model = SimpleMeanPredictor(y_train.mean())\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            r2 = 0  # R² for mean predictor is 0\n",
        "\n",
        "            logger.info(f\"Fallback model performance - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "        # Attempt to get feature importances\n",
        "        try:\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                feature_importances = model.feature_importances_\n",
        "            elif hasattr(model, 'get_feature_importances'):\n",
        "                feature_importances = model.get_feature_importances()\n",
        "            else:\n",
        "                feature_importances = np.ones(X.shape[1]) / X.shape[1]  # Equal importance\n",
        "        except:\n",
        "            feature_importances = np.ones(X.shape[1]) / X.shape[1]  # Equal importance fallback\n",
        "\n",
        "        # Return model and evaluation metrics\n",
        "        return {\n",
        "            'model': model,\n",
        "            'scaler': scaler if 'scaler' in locals() else None,\n",
        "            'metrics': {\n",
        "                'mse': mse,\n",
        "                'rmse': rmse,\n",
        "                'r2': r2\n",
        "            },\n",
        "            'test_data': {\n",
        "                'X_test': X_test,\n",
        "                'y_test': y_test,\n",
        "                'y_pred': y_pred\n",
        "            },\n",
        "            'feature_names': X.columns.tolist(),\n",
        "            'feature_importances': feature_importances\n",
        "        }\n",
        "\n",
        "    def analyze_fairness(self, model_results: Dict, protected_attributes: List[str],\n",
        "                         df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Analyze model fairness across protected attributes.\"\"\"\n",
        "        fairness_results = {}\n",
        "\n",
        "        # If no protected attributes or no model results, return empty results\n",
        "        if not protected_attributes or not model_results:\n",
        "            logger.warning(\"No protected attributes or model results for fairness analysis\")\n",
        "            return fairness_results\n",
        "\n",
        "        # Get test data\n",
        "        try:\n",
        "            X_test = model_results['test_data']['X_test']\n",
        "            y_test = model_results['test_data']['y_test']\n",
        "            y_pred = model_results['test_data']['y_pred']\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Missing test data for fairness analysis: {e}\")\n",
        "            return fairness_results\n",
        "\n",
        "        # For each protected attribute\n",
        "        for attr in protected_attributes:\n",
        "            # Check if attribute is in test data\n",
        "            if attr not in X_test.columns:\n",
        "                logger.warning(f\"Protected attribute '{attr}' not in test data\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Get attribute values\n",
        "                attr_values = X_test[attr].copy()\n",
        "\n",
        "                # Discretize continuous attributes for analysis\n",
        "                if attr_values.dtype.kind in 'fc':  # float or complex\n",
        "                    # Create categories (quintiles)\n",
        "                    try:\n",
        "                        attr_categories = pd.qcut(attr_values, q=5, duplicates='drop')\n",
        "                    except ValueError:\n",
        "                        # Fall back to fewer bins if data isn't diverse enough\n",
        "                        unique_values = attr_values.unique()\n",
        "                        if len(unique_values) < 5:\n",
        "                            # Just use the unique values as categories\n",
        "                            attr_categories = attr_values\n",
        "                        else:\n",
        "                            # Try quantiles with fewer bins\n",
        "                            try:\n",
        "                                attr_categories = pd.qcut(attr_values, q=3, duplicates='drop')\n",
        "                            except:\n",
        "                                # Last resort - just use the raw values\n",
        "                                attr_categories = attr_values\n",
        "                else:\n",
        "                    # Use as-is for categorical\n",
        "                    attr_categories = attr_values\n",
        "\n",
        "                # Calculate error by group\n",
        "                group_errors = {}\n",
        "                overall_abs_error = np.abs(y_test - y_pred).mean()\n",
        "\n",
        "                for category in attr_categories.unique():\n",
        "                    # Get indices for this category\n",
        "                    idx = attr_categories == category\n",
        "                    if idx.sum() == 0:\n",
        "                        continue\n",
        "\n",
        "                    # Calculate error for this group\n",
        "                    group_error = np.abs(y_test[idx] - y_pred[idx]).mean()\n",
        "                    group_errors[str(category)] = group_error\n",
        "\n",
        "                # Calculate disparity metrics\n",
        "                if len(group_errors) > 1:\n",
        "                    # Difference between highest and lowest error\n",
        "                    error_disparity = max(group_errors.values()) - min(group_errors.values())\n",
        "                    # Ratio of highest to lowest error (avoid division by zero)\n",
        "                    min_error = max(min(group_errors.values()), 0.001)\n",
        "                    error_ratio = max(group_errors.values()) / min_error\n",
        "\n",
        "                    fairness_results[attr] = {\n",
        "                        'group_errors': group_errors,\n",
        "                        'error_disparity': error_disparity,\n",
        "                        'error_ratio': error_ratio,\n",
        "                        'overall_error': overall_abs_error\n",
        "                    }\n",
        "\n",
        "                    logger.info(f\"Fairness analysis for {attr}:\")\n",
        "                    logger.info(f\"  Error disparity: {error_disparity:.4f}\")\n",
        "                    logger.info(f\"  Error ratio: {error_ratio:.4f}\")\n",
        "\n",
        "                    # Check for unfairness\n",
        "                    if error_ratio > 1.5:  # Arbitrary threshold\n",
        "                        logger.warning(f\"Potential unfairness detected for {attr} - error ratio: {error_ratio:.2f}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error analyzing fairness for attribute {attr}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return fairness_results\n",
        "\n",
        "    def plot_fairness_results(self, fairness_results: Dict) -> None:\n",
        "        \"\"\"Plot fairness analysis results.\"\"\"\n",
        "        if not fairness_results:\n",
        "            logger.info(\"No fairness results to plot\")\n",
        "            return\n",
        "\n",
        "        # Create a plot for each protected attribute\n",
        "        for attr, results in fairness_results.items():\n",
        "            try:\n",
        "                # Bar plot of errors by group\n",
        "                group_errors = results['group_errors']\n",
        "\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                groups = list(group_errors.keys())\n",
        "                errors = list(group_errors.values())\n",
        "\n",
        "                # Sort by error value\n",
        "                sorted_pairs = sorted(zip(groups, errors), key=lambda x: x[1])\n",
        "                groups, errors = zip(*sorted_pairs)\n",
        "\n",
        "                # Create bar plot\n",
        "                ax = sns.barplot(x=list(groups), y=list(errors))\n",
        "                plt.axhline(y=results['overall_error'], color='r', linestyle='--', label='Overall Error')\n",
        "                plt.xlabel(f'{attr} Group')\n",
        "                plt.ylabel('Mean Absolute Error')\n",
        "                plt.title(f'Error by {attr} Group')\n",
        "                plt.legend()\n",
        "\n",
        "                # Rotate x-axis labels if there are many groups\n",
        "                if len(groups) > 4:\n",
        "                    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save plot if needed\n",
        "                plot_file = os.path.join(self.config.results_dir, f'fairness_{attr}.png')\n",
        "                plt.savefig(plot_file)\n",
        "                logger.info(f\"Saved fairness plot to {plot_file}\")\n",
        "\n",
        "                # Display in notebook\n",
        "                plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error plotting fairness results for {attr}: {e}\")\n",
        "\n",
        "    def plot_feature_importance(self, model_results: Dict) -> None:\n",
        "        \"\"\"Plot feature importance.\"\"\"\n",
        "        if not model_results or 'feature_importances' not in model_results:\n",
        "            logger.info(\"No feature importance to plot\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Get feature names and importances\n",
        "            feature_names = model_results['feature_names']\n",
        "            importances = model_results['feature_importances']\n",
        "\n",
        "            # Sort by importance\n",
        "            sorted_idx = np.argsort(importances)\n",
        "\n",
        "            # Plot only top 20 features if there are many\n",
        "            if len(feature_names) > 20:\n",
        "                sorted_idx = sorted_idx[-20:]\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.barh(range(len(sorted_idx)), importances[sorted_idx])\n",
        "            plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
        "            plt.xlabel('Feature Importance')\n",
        "            plt.title('Top Features by Importance')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save plot\n",
        "            plot_file = os.path.join(self.config.results_dir, 'feature_importance.png')\n",
        "            plt.savefig(plot_file)\n",
        "            logger.info(f\"Saved feature importance plot to {plot_file}\")\n",
        "\n",
        "            # Display in notebook\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error plotting feature importance: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the pipeline.\"\"\"\n",
        "    # Initialize configuration\n",
        "    config = Config()\n",
        "    logger.info(f\"Using {config.n_cores} CPU cores for processing\")\n",
        "    logger.info(\"Starting predictive policing pipeline\")\n",
        "\n",
        "    # Initialize processors\n",
        "    data_processor = DataProcessor(config)\n",
        "\n",
        "    # Load and process data\n",
        "    try:\n",
        "        logger.info(\"Loading input data\")\n",
        "        # Check which methods are available in the DataProcessor class\n",
        "        # Based on the available methods, we'll use the appropriate one\n",
        "        if hasattr(data_processor, 'import_data'):\n",
        "            raw_data = data_processor.import_data(config.input_path)\n",
        "        elif hasattr(data_processor, 'get_data'):\n",
        "            raw_data = data_processor.get_data(config.input_path)\n",
        "        else:\n",
        "            # If no suitable method exists, we'll need to implement one\n",
        "            logger.info(\"No data loading method found, implementing custom loader\")\n",
        "            raw_data = load_data_from_path(config.input_path)\n",
        "\n",
        "        logger.info(\"Preprocessing data\")\n",
        "        processed_data = data_processor.preprocess(raw_data)\n",
        "\n",
        "        # Initialize model\n",
        "        logger.info(\"Initializing prediction model\")\n",
        "        model = PredictionModel(config)\n",
        "\n",
        "        # Train or load model\n",
        "        if config.train_model:\n",
        "            logger.info(\"Training predictive model\")\n",
        "            model.train(processed_data)\n",
        "            logger.info(f\"Saving model to {config.model_path}\")\n",
        "            model.save(config.model_path)\n",
        "        else:\n",
        "            logger.info(f\"Loading model from {config.model_path}\")\n",
        "            model.load(config.model_path)\n",
        "\n",
        "        # Generate predictions\n",
        "        logger.info(\"Generating predictions\")\n",
        "        predictions = model.predict(processed_data)\n",
        "\n",
        "        # Generate reports\n",
        "        logger.info(\"Generating analysis reports\")\n",
        "        report_generator = ReportGenerator(config)\n",
        "        report_generator.create_reports(processed_data, predictions)\n",
        "\n",
        "        # Export results\n",
        "        logger.info(\"Exporting results\")\n",
        "        exporter = ResultExporter(config)\n",
        "        exporter.export(predictions, config.output_path)\n",
        "\n",
        "        logger.info(\"Pipeline completed successfully\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "        logger.debug(traceback.format_exc())\n",
        "        return False\n",
        "\n",
        "# Implement a custom data loading function\n",
        "def load_data_from_path(path):\n",
        "    \"\"\"Load data from the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the data file\n",
        "\n",
        "    Returns:\n",
        "        data: Loaded data object\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading data from {path}\")\n",
        "    # Determine file type based on extension\n",
        "    if path.endswith('.csv'):\n",
        "        import pandas as pd\n",
        "        return pd.read_csv(path)\n",
        "    elif path.endswith('.json'):\n",
        "        import json\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    elif path.endswith('.xlsx') or path.endswith('.xls'):\n",
        "        import pandas as pd\n",
        "        return pd.read_excel(path)\n",
        "    else:\n",
        "        # Generic file reading\n",
        "        with open(path, 'r') as f:\n",
        "            return f.read()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qGVr43TmLJUs",
        "outputId": "2237bd0c-0832-4224-f8ba-937d4c1fda09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:predictive_policing:Pipeline failed: 'Config' object has no attribute 'input_path'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}